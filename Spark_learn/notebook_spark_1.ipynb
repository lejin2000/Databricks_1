{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "298c168b-5743-43f9-963e-58e6f8a6c2eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a DataFrame with test data\n",
    "data = [[2021, \"test\", \"Albany\", \"M\", 42],\n",
    "        [2022, \"test2\", \"Albany2\", \"M\", 43],\n",
    "        [2023, \"test3\", \"Albany3\", \"M\", 44],\n",
    "        [2023, \"test3\", \"Albany4\", \"M\", 50],\n",
    "        [2023, \"test4\", \"Albany5\", \"M\", 51]\n",
    "        ]\n",
    "#columns = [\"Year\", \"First_Name\", \"County\", \"Sex\", \"Count\"]\n",
    "df = spark.createDataFrame(data, schema=\"Year int, First_Name STRING, County STRING, Sex STRING, Count int\")\n",
    "\n",
    "# Display the DataFrame (Databricks-specific method)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "226926bc-5141-4b62-8c20-e2938b5cc03c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Filter rows where Count > 50\n",
    "filtered_df = df.where(df[\"Count\"] >= 50)\n",
    "display(filtered_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5168d90-08cc-4846-adb7-d4f2ac1af2db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Select specific columns and order by frequency\n",
    "from pyspark.sql.functions import asc\n",
    "result = df.select(\"First_Name\", \"County\").orderBy(asc(\"Count\"))\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41949e06-3644-4996-a0bc-380a6c5d62e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a temporary view\n",
    "df.createOrReplaceTempView(\"people\")\n",
    "\n",
    "# Run SQL queries\n",
    "result = spark.sql(\"SELECT First_Name, COUNT(*) FROM people GROUP BY First_Name\")\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05f9d102-9f22-480a-b0ea-d03db748ee1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Access sample data from the samples catalog\n",
    "dfpeople = spark.sql(\"SELECT * FROM people LIMIT 5\")\n",
    "display(dfpeople)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "notebook_spark_1",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
